whooping cough.

boot.dev
Docker:
	Learning about networks still. 
	By providing a name to our two running containers and connecting them to our network we are able to access them through the curl command via their name.
	using this command 
		```docker run --network caddytest --name caddy1 -v $PWD/index1.html:/usr/share/caddy/index.html caddy```
	I run a container with the name caddy1 on the caddytest network. This means that I dont have to define any ports.
	Then I use the following command to start the getting-started container as part of the caddytest network. So now when I am in this container I have access to the two other conatiners as they are on the same network. And because they are named I can just used their names as a URI
		```docker run -it --network caddytest docker/getting-started /bin/sh```

Load Balancing:
Allows us to expose only the load balancer and not the application servers and the load balancer with "balance" the traffic depending on what we define in the Caddyfile.
Using Caddy as a load balancer requires setting up a 'Caddyfile' The contents of the provided file are 
```
localhost:80 # defines the host machine port

reverse_proxy caddy1:80 caddy2:80 { # defines the containers that will be part of the load balancing
	lb_policy       round_robin # defines how the load balancer will work
}
```

Then by running:
```
docker run --network caddytest -p 8080:80 -v $PWD/Caddyfile:/etc/caddy/Caddyfile caddy
```
I set up the network to run another instance of caddy as our loadbalancer since I have our caddy file mounted to do so.
At first it was not sawpping because I had the cached page, so I used a private window that doesnt cache (as much)

Dockerfiles:
	As I learned in school, Docker files define how an image will be built.
```Dockerfile
# This is a comment

# Use a lightweight debian os
# as the base image
FROM debian:stable-slim # Define what BASE image will be used

# execute the 'echo "hello world"'
# command when the container runs
CMD ["echo", "hello world"]	# Defines the entry command that happens when running
```

Dockerizing a GO program:
I used a sample project, a [simple server](https://github.com/CTK-code/simple-server), to dockerize. 
The docker file is super simple.
Uses the slim debian base image
It copies the executable from the built project onto the image

Finally it runs the server. 
```Dockerfile
FROM debian:stable-slim

# COPY source destination
COPY simple-server /bin/goserver

ENV PORT=8080 # defines an environment variable in the image

CMD ["/bin/goserver"]

```
I had an issue here, which is because I am not building on a Linux machine, but my container needs to have a program built for a Linux machine (Im on MacOS)
The solution was to build my go program with this command 
```GOOS=linux GOARCH=amd64 go build``` Which tells the compiler to build to target a linux system instead of for my mac machine.

Containerizing a Python program:
This is different because it is not a compiled program in just a binary.
The initial dockerfile will look like this:
```Dockerfile
FROM debian:stable-slim
COPY main.py main.py
COPY books/ books/
CMD ["python", "main.py"]
```
This does not work because of this error :
``` docker: Error response from daemon: failed to create task for container: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: exec: "python": executable file not found in $PATH: unknown.```
I assume this is because python is not installed in the container.

I was right, this is a working Dockerfile:
```Dockerfile
# Build from a slim Debian/Linux image
FROM debian:stable-slim

# Update apt
RUN apt update
RUN apt upgrade -y

# Install build tooling
RUN apt install -y build-essential zlib1g-dev libncurses5-dev libgdbm-dev libnss3-dev libssl-dev libreadline-dev libffi-dev libsqlite3-dev wget libbz2-dev

# Download Python interpreter code and unpack it
RUN wget https://www.python.org/ftp/python/3.10.8/Python-3.10.8.tgz
RUN tar -xf Python-3.10.*.tgz

# Build the Python interpreter
RUN cd Python-3.10.8 && ./configure --enable-optimizations && make && make altinstall

# Copy our code into the image
COPY main.py main.py

# Copy our data dependencies
COPY books/ books/

# Run our Python script
CMD ["python3.10", "main.py"]
```
- We update apt and upgrade packages.
- install required tooling
- Install python
- Setup configurations for python
- Finally we continue to our original file
NOTE: It has not been touched on yet but it is very important to know that the order here matters. Each of these commands are cached and checked for changes. If there is a change in one command, all commands after are rerun. So having the program copy after the python download means that the python does not get reinstalled after every code change that we make.
I didn't find this lesson very informative, it just felt like showing why go is good, but man I guess go is good.

Latest will be used for the version tag if none is provided. This can cause confusion since our recent 0.2.0 version is the most recently uploaded, but it is not the latest. This can be solved with
```bash
docker build -t username/imagename:0.0.0 -t username/imagename:latest .
docker push username/imagename --all-tags
```
Since this will upload the version but also update our latest version